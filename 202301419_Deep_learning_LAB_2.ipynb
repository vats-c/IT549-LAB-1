{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WCe9VJao_jc",
        "outputId": "32a0aae1-45c8-476a-ea19-8fbe2cbbc58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-20 11:50:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2026-02-20 11:50:54--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2026-02-20 11:50:54--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 39s  \n",
            "\n",
            "2026-02-20 11:53:34 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "!unzip -q glove.6B.zip -d glove"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1 - Data Preparation**"
      ],
      "metadata": {
        "id": "eo29d83ctTrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "df = pd.read_csv('movies.csv')\n",
        "df.rename(columns={'genres': 'genre', 'vote_average': 'voting_average'}, inplace=True)\n",
        "\n",
        "allowed_cols = ['genre', 'keywords', 'tagline', 'overview', 'voting_average']\n",
        "df = df[allowed_cols].dropna()\n",
        "\n",
        "# Text Preprocessing Function\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Clean the input text columns\n",
        "for col in ['overview', 'tagline', 'keywords']:\n",
        "    df[col + '_clean'] = df[col].apply(clean_text)\n",
        "\n",
        "# Format genres into a list\n",
        "df['genre_list'] = df['genre'].apply(lambda x: str(x).split())\n",
        "\n",
        "# Train/Validation/Test Split (70/15/15)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42)\n",
        "\n",
        "print(\"Data splits:\", len(train_df), len(val_df), len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFjw5Y9ltUB-",
        "outputId": "3e2e5c44-c23d-4d03-9896-6bbd28543794"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data splits: 2631 564 564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2 - GloVe Embedding Pipeline**\n",
        "\n",
        "Embedding used: 100d"
      ],
      "metadata": {
        "id": "6_GBcwcSwp_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load 100D GloVe vectors\n",
        "embeddings_index = {}\n",
        "with open('glove/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Report embedding coverage\n",
        "vocab = set(word for tokens in train_df['overview_clean'] for word in tokens)\n",
        "covered = len([w for w in vocab if w in embeddings_index])\n",
        "print(f\"Embedding Coverage: {covered / len(vocab) * 100:.2f}%\")\n",
        "\n",
        "# Construct TF-IDF weighted document embeddings\n",
        "def get_doc_embeddings(df_split, text_col):\n",
        "    tfidf = TfidfVectorizer()\n",
        "    # Fit only on training data to prevent data leakage\n",
        "    tfidf.fit([\" \".join(tokens) for tokens in train_df[text_col]])\n",
        "    word2tfidf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
        "\n",
        "    doc_embeddings = []\n",
        "    for tokens in df_split[text_col]:\n",
        "        vecs, weights = [], []\n",
        "        for word in tokens:\n",
        "            if word in embeddings_index and word in word2tfidf:\n",
        "                vecs.append(embeddings_index[word])\n",
        "                weights.append(word2tfidf[word])\n",
        "\n",
        "        # Calculate weighted average or return zeroes if empty\n",
        "        doc_emb = np.average(vecs, axis=0, weights=weights) if vecs else np.zeros(100)\n",
        "        doc_embeddings.append(doc_emb)\n",
        "    return np.array(doc_embeddings)\n",
        "\n",
        "# Generate embeddings for the 'overview' column\n",
        "X_train = get_doc_embeddings(train_df, 'overview_clean')\n",
        "X_test = get_doc_embeddings(test_df, 'overview_clean')\n",
        "\n",
        "y_train_reg = train_df['voting_average'].values\n",
        "y_test_reg = test_df['voting_average'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVBFn-XVwZ_L",
        "outputId": "20133069-a86c-433f-fe5d-6644c2ee9f4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Coverage: 91.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3 - Model A: Rating Prediction (Regression)**"
      ],
      "metadata": {
        "id": "5FMewq95xUYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "class RegressionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegressionNet, self).__init__()\n",
        "        # Input: 100D GloVe vector -> Hidden Layer: 64 neurons\n",
        "        self.fc1 = nn.Linear(100, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Output: 1 single continuous number (the predicted rating)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# Reusable Training and Evaluation Function\n",
        "def train_and_evaluate_regression(text_col):\n",
        "    print(f\"--- Training Model A (Regression) using: {text_col} ---\")\n",
        "\n",
        "    # Get embeddings for this specific text column using the function from Task 2\n",
        "    X_train_col = get_doc_embeddings(train_df, text_col)\n",
        "    X_test_col = get_doc_embeddings(test_df, text_col)\n",
        "\n",
        "    # Create PyTorch DataLoaders\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train_col, dtype=torch.float32),\n",
        "                                  torch.tensor(y_train_reg, dtype=torch.float32).unsqueeze(1))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize Model, Loss Function (MSE), and Optimizer\n",
        "    model = RegressionNet()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Training Loop\n",
        "    epochs = 15\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0: # Print loss every 5 epochs\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_preds = model(torch.tensor(X_test_col, dtype=torch.float32)).numpy()\n",
        "\n",
        "    # Calculate Metrics\n",
        "    mse = mean_squared_error(y_test_reg, test_preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Baseline: Just predict the mean rating of the training set for every test sample\n",
        "    global_mean = train_df['voting_average'].mean()\n",
        "    baseline_preds = np.full_like(y_test_reg, global_mean)\n",
        "    baseline_mse = mean_squared_error(y_test_reg, baseline_preds)\n",
        "\n",
        "    print(f\"-> Baseline MSE: {baseline_mse:.4f}\")\n",
        "    print(f\"-> Model MSE:    {mse:.4f}\")\n",
        "    print(f\"-> Model RMSE:   {rmse:.4f}\\n\")\n",
        "\n",
        "# 3. Run for two different inputs to compare performance\n",
        "train_and_evaluate_regression('overview_clean')\n",
        "train_and_evaluate_regression('tagline_clean')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBc2niCSxV2G",
        "outputId": "e665038e-d147-4dfd-9f05-eecbb320bc5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Model A (Regression) using: overview_clean ---\n",
            "Epoch 5/15 | Training Loss: 1.1102\n",
            "Epoch 10/15 | Training Loss: 0.9732\n",
            "Epoch 15/15 | Training Loss: 0.8983\n",
            "-> Baseline MSE: 0.8977\n",
            "-> Model MSE:    0.9198\n",
            "-> Model RMSE:   0.9591\n",
            "\n",
            "--- Training Model A (Regression) using: tagline_clean ---\n",
            "Epoch 5/15 | Training Loss: 1.0206\n",
            "Epoch 10/15 | Training Loss: 0.8448\n",
            "Epoch 15/15 | Training Loss: 0.7831\n",
            "-> Baseline MSE: 0.8977\n",
            "-> Model MSE:    1.0071\n",
            "-> Model RMSE:   1.0035\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4 - Model B: Genre Prediction (Multi-Label Classification)**"
      ],
      "metadata": {
        "id": "ryDR3sgC0ejz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, hamming_loss\n",
        "\n",
        "#  Binarize the genres (Convert list of text genres into 1s and 0s)\n",
        "mlb = MultiLabelBinarizer()\n",
        "# Fit on train data to learn all possible genres, then transform both sets\n",
        "y_train_cls = mlb.fit_transform(train_df['genre_list'])\n",
        "y_test_cls = mlb.transform(test_df['genre_list'])\n",
        "\n",
        "num_classes = len(mlb.classes_)\n",
        "print(f\"Total unique genres found: {num_classes}\")\n",
        "\n",
        "# Define the Multi-Label PyTorch Network\n",
        "class MultiLabelNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiLabelNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 128) # 100D GloVe input\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes) # Output size equals number of genres\n",
        "        # There's no need for Sigmoid here as BCEWithLogitsLoss handles it automatically.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# Reusable Training and Evaluation Function\n",
        "def train_and_evaluate_classification(text_col):\n",
        "    print(f\"\\n--- Training Model B (Classification) using: {text_col} ---\")\n",
        "\n",
        "    # Get embeddings for this specific text column\n",
        "    X_train_col = get_doc_embeddings(train_df, text_col)\n",
        "    X_test_col = get_doc_embeddings(test_df, text_col)\n",
        "\n",
        "    # Create PyTorch DataLoaders\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train_col, dtype=torch.float32),\n",
        "                                  torch.tensor(y_train_cls, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize Model, Loss Function (BCEWithLogitsLoss), and Optimizer\n",
        "    model = MultiLabelNet(num_classes)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Training Loop\n",
        "    epochs = 15\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get raw scores\n",
        "        raw_logits = model(torch.tensor(X_test_col, dtype=torch.float32))\n",
        "        # Turn raw scores into percentages (0.0 to 1.0) using sigmoid\n",
        "        predicted_probs = torch.sigmoid(raw_logits)\n",
        "        # If the model is more than 50% sure, call it a 1 (genre present)\n",
        "        predictions = (predicted_probs > 0.5).int().numpy()\n",
        "\n",
        "    # Calculate Required Metrics\n",
        "    micro_f1 = f1_score(y_test_cls, predictions, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(y_test_cls, predictions, average='macro', zero_division=0)\n",
        "    h_loss = hamming_loss(y_test_cls, predictions)\n",
        "\n",
        "    print(f\"-> Micro-F1:     {micro_f1:.4f}\")\n",
        "    print(f\"-> Macro-F1:     {macro_f1:.4f}\")\n",
        "    print(f\"-> Hamming Loss: {h_loss:.4f}\")\n",
        "\n",
        "# 4. Run for two different inputs to compare performance\n",
        "train_and_evaluate_classification('overview_clean')\n",
        "train_and_evaluate_classification('tagline_clean')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uld3kXFH0fK_",
        "outputId": "cadbdd26-ba64-47db-b6d2-aaf80ca3da94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique genres found: 22\n",
            "\n",
            "--- Training Model B (Classification) using: overview_clean ---\n",
            "Epoch 5/15 | Training Loss: 0.2256\n",
            "Epoch 10/15 | Training Loss: 0.2096\n",
            "Epoch 15/15 | Training Loss: 0.1975\n",
            "-> Micro-F1:     0.5232\n",
            "-> Macro-F1:     0.3695\n",
            "-> Hamming Loss: 0.1018\n",
            "\n",
            "--- Training Model B (Classification) using: tagline_clean ---\n",
            "Epoch 5/15 | Training Loss: 0.2679\n",
            "Epoch 10/15 | Training Loss: 0.2478\n",
            "Epoch 15/15 | Training Loss: 0.2253\n",
            "-> Micro-F1:     0.3270\n",
            "-> Macro-F1:     0.1623\n",
            "-> Hamming Loss: 0.1260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 5 - Frequent Words per Genre**"
      ],
      "metadata": {
        "id": "50w0seCO1YlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"--- TASK 5: Frequent Words per Genre ---\")\n",
        "# 1. Set up an empty list for every single genre\n",
        "genre_words = {genre: [] for genre in mlb.classes_}\n",
        "\n",
        "# 2. Go through our movie dataset and throw every word into its matching genre bucket\n",
        "for idx, row in train_df.iterrows():\n",
        "    for genre in row['genre_list']:\n",
        "        genre_words[genre].extend(row['overview_clean'])\n",
        "\n",
        "# Note: We are only printing the first 3 genres here to keep your screen clean.\n",
        "# To see all 22 genres for your assignment table, just remove the '[:3]' below!\n",
        "for genre in list(mlb.classes_)[:3]:\n",
        "    # Count the words, keeping only those that appear at least 3 times\n",
        "    counts = {k: v for k, v in Counter(genre_words[genre]).items() if v >= 3}\n",
        "    if counts:\n",
        "        sorted_w = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(f\"\\nGenre: {genre}\")\n",
        "        print(f\"Top 10: {[w[0] for w in sorted_w[:10]]}\")\n",
        "        print(f\"Bottom 10: {[w[0] for w in sorted_w[-10:]]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0V0DMKl1Y7L",
        "outputId": "10413be4-0966-4176-c4fa-e38cbc2152a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TASK 5: Frequent Words per Genre ---\n",
            "\n",
            "Genre: Action\n",
            "Top 10: ['the', 'a', 'to', 'of', 'and', 'his', 'in', 'is', 'with', 'on']\n",
            "Bottom 10: ['wiley', 'jill', 'seattle', 'thor', 'wade', 'immigrants', 'averill', 'cavendich', 'tao', 'bazil']\n",
            "\n",
            "Genre: Adventure\n",
            "Top 10: ['the', 'a', 'to', 'and', 'of', 'in', 'his', 'is', 'with', 'on']\n",
            "Bottom 10: ['drug', 'taylor', 'thor', 'wade', 'assistant', 'austin', 'cavendich', 'redferne', 'undead', 'alice']\n",
            "\n",
            "Genre: Animation\n",
            "Top 10: ['the', 'a', 'and', 'to', 'of', 'in', 'his', 'is', 'an', 'he']\n",
            "Bottom 10: ['mike', 'penguins', 'planet', 'amadeo', 'table', 'five', 'season', 'turtle', 'kung', 'fu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 6 - Genre-Indicative Words Using TF-IDF**"
      ],
      "metadata": {
        "id": "Y0s5_IJO25nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- TASK 6: Genre-Indicative Words Using TF-IDF ---\")\n",
        "# 1. Set up TF-IDF to find words that are unique, not just common\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform([\" \".join(tokens) for tokens in train_df['overview_clean']])\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "for i, genre in enumerate(list(mlb.classes_)[:10]):\n",
        "    # 2. Train a simple linear model to look for strong connections\n",
        "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    lr.fit(X_tfidf, y_train_cls[:, i])\n",
        "\n",
        "    # 3. Pull out the top 10 words with the highest positive weights\n",
        "    top_indices = lr.coef_[0].argsort()[-10:][::-1]\n",
        "    indicative_words = [feature_names[idx] for idx in top_indices]\n",
        "    print(f\"\\n{genre} Indicative Words: {indicative_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzb8YIyI26Ac",
        "outputId": "7a42772a-a709-45f1-e816-357de7a5d557"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TASK 6: Genre-Indicative Words Using TF-IDF ---\n",
            "\n",
            "Action Indicative Words: ['agent', 'the', 'forces', 'james', 'bond', 'cop', 'battle', 'must', 'against', 'avenge']\n",
            "\n",
            "Adventure Indicative Words: ['the', 'bond', 'adventure', 'find', 'to', 'park', 'captain', 'and', 'must', 'earth']\n",
            "\n",
            "Animation Indicative Words: ['land', 'when', 'adventure', 'human', 'shrek', 'dragon', 'animated', 'garfield', 'world', 'away']\n",
            "\n",
            "Comedy Indicative Words: ['comedy', 'just', 'when', 'big', 'up', 'best', 'all', 'christmas', 'their', 'romance']\n",
            "\n",
            "Crime Indicative Words: ['police', 'drug', 'murder', 'detective', 'cop', 'fbi', 'agent', 'mob', 'mafia', 'criminal']\n",
            "\n",
            "Documentary Indicative Words: ['documentary', 'me', 'look', 'afghanistan', 'part', 'intimate', 'penguins', 'the', 'amanda', 'americans']\n",
            "\n",
            "Drama Indicative Words: ['story', 'father', 'life', 'was', 'his', 'left', 'love', 'her', 'leads', 'he']\n",
            "\n",
            "Family Indicative Words: ['dog', 'adventures', 'boy', 'land', 'the', 'save', 'when', 'christmas', 'world', 'animated']\n",
            "\n",
            "Fantasy Indicative Words: ['evil', 'powers', 'magic', 'ancient', 'vampires', 'dragon', 'prince', 'world', 'witches', 'save']\n",
            "\n",
            "Fiction Indicative Words: ['planet', 'earth', 'alien', 'future', 'powers', 'space', 'world', 'humanity', 'futuristic', 'aliens']\n"
          ]
        }
      ]
    }
  ]
}